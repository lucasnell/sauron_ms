
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\theequation}{A\arabic{equation}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}


\section*{Appendix A: Matrix derivatives in quantitative genetics equations}


\subsection*{Axis change}

The partial derivative of fitness with respect to axes is

\begin{equation*}
\begin{split}
    \frac{ \partial F_{i,t} }{ \partial \mathbf{v}_{i,t} } =
        \exp & \left\{
            r_0
            - f \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{C} \mathbf{v}_{i,t}
            - \alpha_0 \, N_{i,t}
            - \alpha_0 \,
                \textrm{e}^{- \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t}}
            \left(
                \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
                - \mathbf{v}_{j,t}^{\textrm{T}}
                \mathbf{D} \mathbf{v}_{j,t} } }
            \right)
        \right\} \\
        & \left[
            2 \, \alpha_0 \left(
                \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
                - \mathbf{v}_{j,t}^{\textrm{T}}
                \mathbf{D} \mathbf{v}_{j,t} } }
            \right)
                \textrm{e}^{- \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t}} \:
                \mathbf{v}_{i,t}^{\textrm{T}}
            - 2 f \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{C}
        \right]
    \textrm{.}
\end{split}
\end{equation*}


This, combined with equation \ref{eq:axis-change}, means that axes change as follows:

\begin{equation} \label{eq:axes-change-full}
    \mathbf{v}_{i,t+1} = \mathbf{v}_{i,t} + 2 \sigma_A^2
    \left[
        \alpha_0 \, \left(
            \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
            - \mathbf{v}_{j,t}^{\textrm{T}}
            \mathbf{D} \mathbf{v}_{j,t} } }
        \right) \:
            \textrm{e}^{- \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t}} \:
            \mathbf{v}_{i,t}^{\textrm{T}}
        - f \, \mathbf{v}_{i,t}^{\textrm{T}} \: \mathbf{C}
    \right]
    \textrm{.}
\end{equation}


\subsection*{Jacobian}

The Jacobian is

\begin{equation} \label{eq:jacobian}
    \begin{pmatrix}
        \frac{\partial \mathbf{v}_{1,t+1}}{\partial \mathbf{v}_{1,t}} & \cdots &
            \frac{\partial \mathbf{v}_{1,t+1}}{\partial \mathbf{v}_{n,t}} &
            \frac{\partial \mathbf{v}_{1,t+1}}{\partial N_{1,t}} & \cdots &
            \frac{\partial \mathbf{v}_{1,t+1}}{\partial N_{n,t}} \\
        \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial \mathbf{v}_{n,t+1}}{\partial \mathbf{v}_{1,t}} & \cdots &
            \frac{\partial \mathbf{v}_{n,t+1}}{\partial \mathbf{v}_{n,t}} &
            \frac{\partial \mathbf{v}_{n,t+1}}{\partial N_{1,t}} & \cdots &
            \frac{\partial \mathbf{v}_{n,t+1}}{\partial N_{n,t}} \\[1ex]
%
%
        \frac{\partial N_{1,t+1}}{\partial \mathbf{v}_{1,t}} & \cdots &
            \frac{\partial N_{1,t+1}}{\partial \mathbf{v}_{n,t}} &
            \frac{\partial N_{1,t+1}}{\partial N_{1,t}} & \cdots &
            \frac{\partial N_{1,t+1}}{\partial N_{n,t}} \\
        \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial N_{n,t+1}}{\partial \mathbf{v}_{1,t}} & \cdots &
            \frac{\partial N_{n,t+1}}{\partial \mathbf{v}_{n,t}} &
            \frac{\partial N_{n,t+1}}{\partial N_{1,t}} & \cdots &
            \frac{\partial N_{n,t+1}}{\partial N_{n,t}} \\
    \end{pmatrix}
    \text{.}
\end{equation}


It is made up of the following derivatives:

\begin{equation*}
\begin{split}
    \frac{ \partial \, \mathbf{v}_{i,t+1} }{ \partial \, \mathbf{v}_{i,t} } &= 
    \mathbf{I} + 2 ~ \sigma_A^2 ~
        \left[
            \alpha_0 ~ \left(
                \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
                - \mathbf{v}_{j,t}^{\textrm{T}}
                \mathbf{D} \mathbf{v}_{j,t} } }
            \right) ~ \textrm{e}^{ - \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t} }
            \left(
                \mathbf{I} - 2 ~ \mathbf{v}_{i,t} \mathbf{v}_{i,t}^{\textrm{T}}
            \right) -
            f \: \mathbf{C}^{\textrm{T}}
        \right] \\\
% 
    \frac{ \partial \: \mathbf{v}_{i,t+1} }{ \partial \: \mathbf{v}_{k,t}} &=
        -4 \; \sigma_A^2 \; \alpha_0 \; N_{k,t} \; \mathbf{v}_{i,t} \;
        \textrm{e}^{
                    - \mathbf{v}_{k,t}^{\textrm{T}} \mathbf{D} \mathbf{v}_{k,t}
                    - \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t}
                } \;
        \mathbf{v}_{k,t}^{\textrm{T}} \; \mathbf{D} \\
% 
    \frac{ \partial \: \mathbf{v}_{i,t+1} }{ \partial \: N_{i,t} } &= 0 \\
    \frac{ \partial \: \mathbf{v}_{i,t+1} }{ \partial \: N_{k,t} } &=
        2 \; \sigma_A^2 \; \alpha_0 \; \mathbf{v}_{i,t} \;
        \textrm{e}^{ - \mathbf{v}_{k,t}^{\textrm{T}} \mathbf{D} \mathbf{v}_{k,t}
            - \mathbf{v}_{i,t}^{\textrm{T}} \mathbf{v}_{i,t} } \\
% 
    \frac{ \partial N_{i,t+1} }{ \partial \mathbf{v}_{i,t} } &= 
        2 \, F_{i,t+1} \,  N_{i,t}
        \left[
            \alpha_0 \, \left(
                \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
                - \mathbf{v}_{j,t}^{\textrm{T}}
                \mathbf{D} \mathbf{v}_{j,t} } }
            \right) \, \text{e}^{ -\mathbf{v}_{i,t}^{\text{T}}
            \mathbf{v}_{i,t} } \, \mathbf{v}_{i,t}^{\text{T}}
            - f \, \mathbf{v}_{i,t}^{\text{T}} \, \mathbf{C}
        \right] \\
    \frac{ \partial N_{i,t+1} }{ \partial \mathbf{v}_{k,t} } &= 
        2 \, F_{i,t+1} \, N_{i,t} \, N_{k,t} \, \alpha_0 \: 
        \text{e}^{ -\mathbf{v}_{i,t}^{\text{T}} \mathbf{v}_{i,t} -
            \mathbf{v}_{k,t}^{\text{T}} \mathbf{D} \mathbf{v}_{k,t} } \:
        \mathbf{v}_{k,t}^{\text{T}} \, \mathbf{D} \\
% 
    \frac{ \partial N_{i,t+1} }{ \partial N_{i,t} } &= 
        F_{i,t+1} \left( 1 - \alpha_0 \: N_{i,t} \right) \\
    \frac{ \partial N_{i,t+1} }{ \partial N_{k,t} } &= 
        - F_{i,t+1} \: N_{i,t} \: \alpha_0 \: 
        \text{e}^{ -\mathbf{v}_{i,t}^{\text{T}} \mathbf{v}_{i,t} -
            \mathbf{v}_{k,t}^{\text{T}} \mathbf{D} \mathbf{v}_{k,t} } 
    \textrm{.}
\end{split}
\end{equation*}



\subsection*{Keeping axes non-negative}

To keep axes $\ge 0$, we used the ramp function, $R(x)$, which is defined as

\begin{equation*}
    R(x) = \begin{cases}
        x & \text{if}\ x \ge 0 \\
        0 & \text{if}\ x < 0
        \end{cases}
    \text{.}
\end{equation*}


\noindent Its derivative is the Heaviside step function:

\begin{equation*}
\begin{split}
    R'(x) &= H(x) \\
    H(x) &= \begin{cases}
        0 & \text{if}\ x \le 0 \\
        1 & \text{if}\ x > 0
        \end{cases}
    \text{.}
\end{split}
\end{equation*}

% Also see https://see.stanford.edu/materials/lsoftaee261/book-fall-07.pdf
% and https://mathworld.wolfram.com/RampFunction.html

Thus, all axis derivatives of the form $\partial \mathbf{v} / \partial x$
are changed to $H(\mathbf{v}(x)) \: \partial \mathbf{v} / \partial x$ to 
account for axes being non-negative.






\subsection*{Effects of stochasticity on axis evolution}

When there is stochasticity in axis evolution,
the second order Taylor series approximation for the expected
value of axis 1 at time $t+1$ is

\begin{equation}
\label{eq:taylor-expansion-final}
\begin{split}
    \text{E}(v_{1,t+1}) & \approx
        v_{1,t} + 2 \; \sigma_A^2 \; \Bigg\{ \\
            & \alpha_0
            \left(
                \sum_{j \ne i}^{n}{ N_{j,t} \, \textrm{e}^{
                    - \mathbf{\ddot{v}}_{j,t}^{\textrm{T}}
                    \mathbf{D} \, \mathbf{\ddot{v}}_{j,t} } }
            \right)
            v_{1,t} \; \text{e}^{-v_{1,t}^2 - v_{2,t}^2} 
            \bigg[ 
                1 + \sigma^2_{\varepsilon_{v_1}} \left( \frac{1}{2} - 4 \; v_{1,t}^2 \right) \\
                & \hspace*{6em} - 2 \; \sigma^2_{\varepsilon_{v_2}} \, v_{2,t}^2 \left( 1 - v_{2,t}^2 \right)
            \bigg] \\
            & - f \bigg[
                v_{1,t} + \eta \; v_{2,t} + \frac{1}{2} \left(
                    \sigma^2_{\varepsilon_{v_1}} \, v_{1,t} + \sigma^2_{\varepsilon_{v_2}} \, \eta \; v_{2,t}
                \right)
            \bigg]
        \Bigg\}
\text{,}
\end{split}
\end{equation}

where we drop the index for species to reduce clutter.
Subscripts for axes are reversed for the approximation for axis 2.


